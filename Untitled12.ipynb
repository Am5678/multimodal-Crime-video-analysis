{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr16DMy9t5sudJz9ixQ30p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Am5678/multimodal-Crime-video-analysis/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoWCheN12prv"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import IPython.display as ipd\n",
        "import librosa.display\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mutagen\n"
      ],
      "metadata": {
        "id": "tr8G-SAJ2scY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mutagen\n",
        "import mutagen.wave"
      ],
      "metadata": {
        "id": "aLq-UxHX2urD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv('data.csv')\n"
      ],
      "metadata": {
        "id": "J-MSO1F42xqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata.head()\n",
        "class_distr = metadata['class'].value_counts()\n",
        "print(class_distr)\n"
      ],
      "metadata": {
        "id": "lLIjLLoV20Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot class distributions\n",
        "class_distr = metadata['class'].value_counts()\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(class_distr.index, class_distr.values, alpha=0.8)\n",
        "plt.title('Class Distribution')\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Class Name', fontsize=12)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gV61ZWcP26yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the DataFrame\n",
        "missing_values = metadata.isnull().sum()\n",
        "\n",
        "# Print the results\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "bWG0-xO52_0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_df(csv_file):\n",
        "    dataset_df = pd.read_csv(csv_file)\n",
        "    filepaths = []\n",
        "    for i, row in dataset_df.iterrows():\n",
        "        filepaths.append(os.path.join('audio', 'fold'+str(row['fold']), row['slice_file_name']))\n",
        "    dataset_df['filepath'] = filepaths\n",
        "    return dataset_df"
      ],
      "metadata": {
        "id": "DKZxiyeV3FKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df = create_dataset_df('data.csv')\n"
      ],
      "metadata": {
        "id": "uHUH3YOJ3Ha6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_audio_metadata_mutagen(filepath):\n",
        "    metadata = {}\n",
        "    f = mutagen.wave.WAVE(filepath)\n",
        "    metadata['length'] = f.info.length\n",
        "    metadata['bitrate'] = f.info.bitrate\n",
        "    metadata['channels'] = f.info.channels\n",
        "    metadata['sample_rate'] = f.info.sample_rate\n",
        "    metadata['bits_per_sample'] = f.info.bits_per_sample\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def compute_audio_statistics(dataset_df):\n",
        "    metadata_dict = {'length': [], 'bitrate': [], 'channels': [], 'sample_rate': [], 'bits_per_sample': []}\n",
        "    # Extract metadata\n",
        "    for filepath in dataset_df['filepath']:\n",
        "        metadata = get_audio_metadata_mutagen(filepath)\n",
        "        for key in metadata_dict.keys():\n",
        "            metadata_dict[key].append(metadata[key])\n",
        "    # Add new columns to dataframe\n",
        "    for key in metadata_dict.keys():\n",
        "        dataset_df[key] = metadata_dict[key]\n",
        "\n",
        "    return dataset_df"
      ],
      "metadata": {
        "id": "KuxNVL4L3Ky7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df = metadata_df.drop(columns=['fold', 'slice_file_name', 'fsID', 'start', 'end'])\n",
        "audio_statistics_df = compute_audio_statistics(metadata_df)\n",
        "audio_statistics_df.describe()\n"
      ],
      "metadata": {
        "id": "x3-Ujqrc3NPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_statistics_df['sample_rate'].value_counts(), audio_statistics_df['bits_per_sample'].value_counts()\n",
        "audio_statistics_df.groupby('class').describe()\n",
        "# Check for outliers using boxplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='class', y='start', data=metadata)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AITk7BKy3TYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot audio file duration distributions\n",
        "fig, ax = plt.subplots()\n",
        "durations = []\n",
        "for i in range(len(metadata)):\n",
        "    file_path = os.path.join('audio', 'fold'+str(metadata.loc[i, 'fold']), metadata.loc[i, 'slice_file_name'])\n",
        "    signal, sr = librosa.load(file_path, sr=None, mono=True)\n",
        "    durations.append(signal.shape[0]/sr)\n",
        "ax.hist(durations, bins=20)\n",
        "ax.set(title='Audio File Duration Distribution', xlabel='Duration (s)', ylabel='Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JrumeiXS3Z2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listen to a few audio files\n",
        "for i in range(5):\n",
        "    file_path = os.path.join('audio', 'fold'+str(metadata.loc[i, 'fold']), metadata.loc[i, 'slice_file_name'])\n",
        "    signal, sr = librosa.load(file_path, sr=None, mono=True)\n",
        "    print('Playing audio file:', metadata.loc[i, 'slice_file_name'])\n",
        "    librosa.display.waveshow(signal, sr=sr)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MpOQRVts3caL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'audio/fold1/7383-3-0-0.wav'\n",
        "fs, signal = wavfile.read(file_path)\n",
        "plt.plot(signal)\n",
        "plt.title('Waveform')\n",
        "plt.xlabel('Time (samples)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tDDAHeuT3hLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MFCC\n",
        "from tqdm import tqdm\n",
        "# Extracting MFCC's For every audio file\n",
        "def features_extractor(file_name):\n",
        "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
        "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
        "\n",
        "    return mfccs_scaled_features"
      ],
      "metadata": {
        "id": "g1SPFBy93lPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dataset_path = 'audio'\n",
        "extracted_features=[]\n",
        "for index_num,row in tqdm(metadata.iterrows()):\n",
        "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
        "    final_class_labels=row[\"class\"]\n",
        "    data=features_extractor(file_name)\n",
        "    extracted_features.append([data,final_class_labels])"
      ],
      "metadata": {
        "id": "1HDqNVeK3q26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting extracted_features to Pandas dataframe\n",
        "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
        "extracted_features_df.head(10)"
      ],
      "metadata": {
        "id": "Oj0Xg-RF3wp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_features_df.to_csv(\"extracted_features.csv\")\n",
        "# Data Splitting\n",
        "X=np.array(extracted_features_df['feature'].tolist())\n",
        "y=np.array(extracted_features_df['class'].tolist())"
      ],
      "metadata": {
        "id": "ttPQskeW3zVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape\n"
      ],
      "metadata": {
        "id": "mgV8pfH435C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y\n"
      ],
      "metadata": {
        "id": "odJ4j_A-38Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder=LabelEncoder()\n",
        "y=to_categorical(labelencoder.fit_transform(y))"
      ],
      "metadata": {
        "id": "fmdf01R63-a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y\n"
      ],
      "metadata": {
        "id": "l-UatJ3P4BDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Testing Sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "6W6DtNFG4Ger"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "PlLQoMA84I3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "2cJtuKWa4Oh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels=y.shape[1]\n",
        "print(num_labels)"
      ],
      "metadata": {
        "id": "n4jifpRf4TEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANN\n",
        "model=Sequential()\n",
        "#first layer\n",
        "model.add(Dense(1600,input_shape=(40,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#second layer\n",
        "model.add(Dense(800))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#third layer\n",
        "model.add(Dense(400))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#final layer\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "_MDHoLMw4WUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
        "# Model training\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime\n",
        "num_epochs = 100\n",
        "num_batch_size = 128\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.h5', verbose=1, save_best_only=True)\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
        "history = model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "metadata": {
        "id": "Kpmbj_i54bfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Er-MY87b4muo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class_label(filename, model_name):\n",
        "    audio, sample_rate = librosa.load(filename, res_type='kaiser_fast')\n",
        "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
        "    mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
        "    predicted_label = np.argmax(model_name.predict(mfccs_scaled_features), axis=-1)\n",
        "    print('Predicted Label:',predicted_label)\n",
        "    prediction_class = labelencoder.inverse_transform(predicted_label)\n",
        "    prediction_class[0]\n",
        "    return prediction_class[0]"
      ],
      "metadata": {
        "id": "_OS6QXvC4pC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_class_label(\"dog_barking.wav\", model)\n",
        "predict_class_label(\"gun_1.wav\", model)\n",
        "predict_class_label(\"siren.wav\", model)\n",
        "predict_class_label(\"car.wav\", model)\n"
      ],
      "metadata": {
        "id": "0SIIDPvV4rHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa.filters\n",
        "def features_extractor_spectral_substraction(file_name):\n",
        "    # Load the audio file\n",
        "    audio, sample_rate = librosa.load(file_name, mono=True, sr=22050)\n",
        "\n",
        "    # Apply preprocessing steps\n",
        "\n",
        "    # Apply preemphasis\n",
        "    audio_filtered = librosa.effects.preemphasis(audio, coef=0.95)\n",
        "\n",
        "\n",
        "\n",
        "    # Extract MFCC features from the preprocessed audio\n",
        "    mfccs_features = librosa.feature.mfcc(y=audio_filtered, sr=sample_rate, n_mfcc=40, n_fft=1024)\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T, axis=0)\n",
        "\n",
        "    # Normalize the features\n",
        "    mfccs_scaled_features = (mfccs_scaled_features - np.mean(mfccs_scaled_features)) / np.std(mfccs_scaled_features)\n",
        "\n",
        "    return mfccs_scaled_features"
      ],
      "metadata": {
        "id": "66_rd_Uj429U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features extraction from all audio files (MFCC)\n",
        "extracted_features_subs=[]\n",
        "for index_num,row in tqdm(metadata.iterrows()):\n",
        "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
        "    final_class_labels=row[\"class\"]\n",
        "    data=features_extractor_spectral_substraction(file_name)\n",
        "    extracted_features_subs.append([data,final_class_labels])"
      ],
      "metadata": {
        "id": "CjBLZ4vC4732"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting extracted_features to Pandas dataframe\n",
        "extracted_features_df=pd.DataFrame(extracted_features_subs,columns=['feature','class'])\n",
        "extracted_features_df.head(10)"
      ],
      "metadata": {
        "id": "NNvzgQL049-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Frame Saving\n",
        "extracted_features_df.to_csv(\"extracted_features_processed.csv\")"
      ],
      "metadata": {
        "id": "vUA0zmJq5Alz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Splitting\n",
        "X=np.array(extracted_features_df['feature'].tolist())\n",
        "y=np.array(extracted_features_df['class'].tolist())"
      ],
      "metadata": {
        "id": "DZwdGqtg5C78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y\n"
      ],
      "metadata": {
        "id": "Toqfukg-5FWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder=LabelEncoder()\n",
        "y=to_categorical(labelencoder.fit_transform(y))"
      ],
      "metadata": {
        "id": "p5oF16TS5ICc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y\n"
      ],
      "metadata": {
        "id": "5PeiB3jD5KWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Testing Sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "PVuPKJqY5M-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2=Sequential()\n",
        "#first layer\n",
        "model_2.add(Dense(1600,input_shape=(40,)))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(Dropout(0.5))\n",
        "#second layer\n",
        "model_2.add(Dense(800))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(Dropout(0.5))\n",
        "#third layer\n",
        "model_2.add(Dense(400))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(Dropout(0.5))\n",
        "\n",
        "#final layer\n",
        "model_2.add(Dense(num_labels))\n",
        "model_2.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "kVHyubH75QoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()\n",
        "!pip install statsmodels\n"
      ],
      "metadata": {
        "id": "ueMl1wJ05TLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.api import SimpleExpSmoothing\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Compile the model with categorical crossentropy loss, accuracy metric, and Adam optimizer\n",
        "model_2.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# Define the number of epochs and batch size\n",
        "num_epochs = 100\n",
        "num_batch_size = 128\n",
        "\n",
        "# Define callbacks for saving the best model and early stopping\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification_high.h5', verbose=1, save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "# Start the timer\n",
        "start = datetime.now()\n",
        "\n",
        "# Fit the model with training data and validation data, using callbacks for saving the best model and early stopping\n",
        "history = model_2.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer, early_stopping], verbose=1)\n",
        "\n",
        "# Calculate the duration of the training\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)\n",
        "\n",
        "# Calculate moving averages of loss and accuracy\n",
        "window_size = 10\n",
        "loss_rolling = pd.Series(history.history['loss']).rolling(window_size).mean().tolist()\n",
        "accuracy_rolling = pd.Series(history.history['accuracy']).rolling(window_size).mean().tolist()\n",
        "\n",
        "# Apply exponential smoothing to loss and accuracy\n",
        "alpha = 0.3\n",
        "loss_smoothed = SimpleExpSmoothing(history.history['loss']).fit(smoothing_level=alpha).fittedvalues.tolist()\n",
        "accuracy_smoothed = SimpleExpSmoothing(history.history['accuracy']).fit(smoothing_level=alpha).fittedvalues.tolist()"
      ],
      "metadata": {
        "id": "dcoDfFjt5ZQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z28RltTQ5enl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Predict the class probabilities for the test data\n",
        "y_prob = model_2.predict(X_test)\n",
        "\n",
        "# Convert the probabilities to class labels\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Calculate the sensitivity and specificity\n",
        "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "\n",
        "# Calculate the classification report\n",
        "class_report = classification_report(y_true, y_pred)\n",
        "\n",
        "# Calculate the loss\n",
        "loss, accuracy = model_2.evaluate(X_test, y_test)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
        "\n",
        "# Print the results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"Sensitivity:\", sensitivity)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"AUC:\", auc)"
      ],
      "metadata": {
        "id": "RUNibl5r5il4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the class names\n",
        "class_names = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n",
        "\n",
        "# Plot the confusion matrix\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, cmap='Purples', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wRSjLaYh5n7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_class_label(\"gun_1.wav\", model_2)\n"
      ],
      "metadata": {
        "id": "uRYvtD_-DHXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "umr_TicuDNAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resnet\n",
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ],
      "metadata": {
        "id": "ZHNVHUbEDNDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the path to the UrbanSound8K dataset\n",
        "data_dir = 'UrbanSound8K/audio'\n",
        "\n",
        "# read the metadata CSV file into a Pandas DataFrame\n",
        "metadata = pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
        "\n",
        "output_dir = 'UrbanSound8K/spectrograms'\n",
        "\n",
        "# create a dictionary to map file names to class labels\n",
        "label_dict = {class_name: i for i, class_name in enumerate(metadata['class'].unique())}\n",
        "\n",
        "# split the metadata into 10 subsets\n",
        "metadata_splits = np.array_split(metadata, 10)\n",
        "\n",
        "# loop through all audio files in each subset with a progress bar\n",
        "for split_index, metadata_split in enumerate(metadata_splits):\n",
        "    for index, row in tqdm(metadata_split.iterrows(), total=len(metadata_split), desc=f\"Processing subset {split_index+1}/10\"):\n",
        "        file_name = row['slice_file_name']\n",
        "        class_name = row['class']\n",
        "        label = label_dict[class_name]\n",
        "        path = os.path.join(data_dir, f'fold{row[\"fold\"]}', file_name)\n",
        "\n",
        "        # load audio file and extract features\n",
        "        y, sr = librosa.load(path, sr=None, duration=4, mono=True, res_type='kaiser_fast') # load audio file with no resampling and duration of 4 seconds\n",
        "        y = librosa.effects.trim(y, top_db=20)[0] # remove silent portions\n",
        "        ## y = librosa.decompose.nn_filter(y, aggregate=np.median, metric='cosine', width=int(librosa.time_to_samples(0.025))) # noise reduction using nn_filter\n",
        "        y = librosa.util.normalize(y) # normalize audio\n",
        "\n",
        "        ft = np.abs(librosa.stft(y)) # calculate Fourier Transform (FT) magnitude\n",
        "        mfccs = librosa.feature.mfcc(S=librosa.power_to_db(ft), n_mfcc=20) # calculate MFCCs from FT magnitude\n",
        "\n",
        "        # plot MFCC spectrogram and save as image\n",
        "        fig, ax = plt.subplots()\n",
        "        img = librosa.display.specshow(mfccs, x_axis='time', ax=ax)\n",
        "        fig.colorbar(img, ax=ax)\n",
        "        ax.set(title='MFCC Spectrogram', xlabel='Time', ylabel='MFCC')\n",
        "        plt.savefig(os.path.join(output_dir, f'{class_name}_{file_name[:-4]}.png'), bbox_inches='tight', pad_inches=0)\n",
        "        plt.close(fig)\n",
        "\n",
        "print(f\"Number of images saved: {len(os.listdir(output_dir))}\")"
      ],
      "metadata": {
        "id": "v0dtXMNVDTbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"spectrograms\"\n",
        "\n",
        "# Loop through all image files in the dataset folder\n",
        "for file_name in os.listdir(dataset_path):\n",
        "    # Get the class name from the file name\n",
        "    class_name = file_name.split(\"_\")[0]\n",
        "    # Define the new folder path for this class\n",
        "    class_folder = os.path.join(dataset_path, class_name)\n",
        "    # If the folder doesn't exist, create it\n",
        "    if not os.path.exists(class_folder):\n",
        "        os.makedirs(class_folder)\n",
        "    # Define the path to the current file\n",
        "    file_path = os.path.join(dataset_path, file_name)\n",
        "    # Define the path to the new file location in the class folder\n",
        "    new_file_path = os.path.join(class_folder, file_name)\n",
        "    # Move the file to the new folder\n",
        "    shutil.move(file_path, new_file_path)"
      ],
      "metadata": {
        "id": "BwxNJqKTDWx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "# Define the path to your dataset\n",
        "dataset_path = \"spectrograms\"\n",
        "\n",
        "# Define the path to the directory to store the training, validation, and test sets\n",
        "train_path = \"train\"\n",
        "valid_path = \"valid\"\n",
        "test_path = \"test\"\n",
        "\n",
        "# Define the train/validation/test split ratios\n",
        "train_ratio = 0.7\n",
        "valid_ratio = 0.2\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Create the subdirectories in the training, validation, and test directories\n",
        "for split_path in [train_path, valid_path, test_path]:\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        if not class_name.startswith('.'):\n",
        "            os.makedirs(os.path.join(split_path, class_name), exist_ok=True)\n",
        "\n",
        "# Loop through each class folder and split the images into train/validation/test sets\n",
        "for class_name in os.listdir(dataset_path):\n",
        "    if not class_name.startswith('.'):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        images = os.listdir(class_path)\n",
        "        images = [image for image in images if not image.startswith('.')]\n",
        "        num_images = len(images)\n",
        "\n",
        "        # Shuffle the images\n",
        "        random.shuffle(images)\n",
        "\n",
        "        # Split the images into train/validation/test sets\n",
        "        num_train = int(num_images * train_ratio)\n",
        "        num_valid = int(num_images * valid_ratio)\n",
        "        num_test = num_images - num_train - num_valid\n",
        "\n",
        "        train_images = images[:num_train]\n",
        "        valid_images = images[num_train:num_train+num_valid]\n",
        "        test_images = images[num_train+num_valid:]\n",
        "\n",
        "        # Copy the images into their respective split folders\n",
        "        for image in train_images:\n",
        "            src_path = os.path.join(class_path, image)\n",
        "            dst_path = os.path.join(train_path, class_name, image)\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "        for image in valid_images:\n",
        "            src_path = os.path.join(class_path, image)\n",
        "            dst_path = os.path.join(valid_path, class_name, image)\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "        for image in test_images:\n",
        "            src_path = os.path.join(class_path, image)\n",
        "            dst_path = os.path.join(test_path, class_name, image)\n",
        "            shutil.copy(src_path, dst_path)"
      ],
      "metadata": {
        "id": "dtsbELFnDaUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the paths to the training, validation, and test directories\n",
        "train_path = 'train'\n",
        "valid_path = 'valid'\n",
        "test_path = 'test'\n",
        "\n",
        "# Define the image size and batch size\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Define the pre-trained model and freeze its layers\n",
        "pretrained_model = ResNet50(weights='imagenet', include_top=False, input_shape=img_size + (3,))\n",
        "for layer in pretrained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(pretrained_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define the data generators for training, validation, and test sets\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(train_path, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_generator = valid_datagen.flow_from_directory(valid_path, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(test_path, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=280, validation_data=valid_generator)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.evaluate(test_generator)\n",
        "model.save(\"ResNet50_450epochs.h5\")"
      ],
      "metadata": {
        "id": "NZLtZwNYDg7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aAqY0AzeDkjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, matthews_corrcoef\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Calculate classification report and confusion matrix\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_true, y_pred))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# Calculate AUC score\n",
        "y_pred_proba = model.predict(test_generator)\n",
        "print('AUC Score:', roc_auc_score(test_generator.labels, y_pred_proba, multi_class='ovr'))\n",
        "\n",
        "# Calculate correlation matrix\n",
        "print('Correlation Matrix:')\n",
        "print(np.corrcoef(y_true, y_pred))\n",
        "print('Matthews Correlation Coefficient:', matthews_corrcoef(y_true, y_pred))"
      ],
      "metadata": {
        "id": "6mZPSbKiDrqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# Define the class names\n",
        "class_names = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n",
        "\n",
        "# Generate predictions for the test set\n",
        "y_pred = model.predict(test_generator).argmax(axis=1)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names, rotation=45)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Fqbz8XqDuaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the data generators for training, validation, and test sets\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(train_path, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_generator = valid_datagen.flow_from_directory(valid_path, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(test_path, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "# Load the pre-trained model\n",
        "pretrained_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Unfreeze the last few layers of the pre-trained model\n",
        "for layer in pretrained_model.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Define the model architecture\n",
        "model2 = Sequential()\n",
        "model2.add(pretrained_model)\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(1024, activation='relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Define the learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 500:\n",
        "        return 0.001\n",
        "    elif epoch < 1000:\n",
        "        return 0.0001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Define the callbacks\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Compile the model\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with the callbacks\n",
        "history = model2.fit(train_generator, epochs=1500, validation_data=valid_generator, callbacks=[lr_scheduler, early_stopping])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model2.evaluate(test_generator)\n",
        "\n",
        "model2.save(\"ResNet50_erl.h5\")"
      ],
      "metadata": {
        "id": "GGG8yThaDxXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy over epochs\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WbaBNiY0Dzv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mfcc_spectrogram(audio_path, output_dir):\n",
        "    y, sr = librosa.load(audio_path, sr=None, duration=4, mono=True, res_type='kaiser_fast')\n",
        "    y = librosa.effects.trim(y, top_db=20)[0]\n",
        "    y = librosa.util.normalize(y)\n",
        "    ft = np.abs(librosa.stft(y))\n",
        "    mfccs = librosa.feature.mfcc(S=librosa.power_to_db(ft), n_mfcc=20)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    img = librosa.display.specshow(mfccs, x_axis='time', ax=ax)\n",
        "    fig.colorbar(img, ax=ax)\n",
        "    ax.set(title='MFCC Spectrogram', xlabel='Time', ylabel='MFCC')\n",
        "    image_path = os.path.join(output_dir, os.path.basename(audio_path)[:-4] + '.png')\n",
        "    plt.savefig(image_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "\n",
        "    return image_path"
      ],
      "metadata": {
        "id": "n-X7A1YeD1qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = 'car.wav'\n",
        "output_dir = '/home/msc1/Desktop/ASB /'\n",
        "image_path = get_mfcc_spectrogram(audio_path, output_dir)"
      ],
      "metadata": {
        "id": "LJc-ZXQUD4BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "\n",
        "# Load the image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "image = img_to_array(image)\n",
        "\n",
        "# Preprocess the image\n",
        "image /= 255.0\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Make a prediction using the model\n",
        "prediction = model.predict(image)\n",
        "\n",
        "# Assuming the prediction output is stored in the variable 'prediction'\n",
        "predicted_class = np.argmax(prediction)\n",
        "\n",
        "# To get the name of the predicted class, you can use the 'class_indices' attribute of the generator used for inference\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "predicted_class_name = class_names[predicted_class]\n",
        "\n",
        "print(predicted_class_name)"
      ],
      "metadata": {
        "id": "mDCnAW0AD6mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "\n",
        "# Load the image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "image = img_to_array(image)\n",
        "\n",
        "# Preprocess the image\n",
        "image /= 255.0\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Make a prediction using the model\n",
        "prediction = model2.predict(image)\n",
        "\n",
        "# Assuming the prediction output is stored in the variable 'prediction'\n",
        "predicted_class = np.argmax(prediction)\n",
        "\n",
        "# To get the name of the predicted class, you can use the 'class_indices' attribute of the generator used for inference\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "predicted_class_name = class_names[predicted_class]\n",
        "\n",
        "print(predicted_class_name)"
      ],
      "metadata": {
        "id": "5jQLAquHD9CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = 'gun_1.wav'\n",
        "output_dir = '/home/msc1/Desktop/ASB /'\n",
        "image_path = get_mfcc_spectrogram(audio_path, output_dir)"
      ],
      "metadata": {
        "id": "BZ1V2cL3D_Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "\n",
        "# Load the image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "image = img_to_array(image)\n",
        "\n",
        "# Preprocess the image\n",
        "image /= 255.0\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Make a prediction using the model\n",
        "prediction = model.predict(image)\n",
        "\n",
        "# Assuming the prediction output is stored in the variable 'prediction'\n",
        "predicted_class = np.argmax(prediction)\n",
        "\n",
        "# To get the name of the predicted class, you can use the 'class_indices' attribute of the generator used for inference\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "predicted_class_name = class_names[predicted_class]\n",
        "\n",
        "print(predicted_class_name)"
      ],
      "metadata": {
        "id": "jSc_F1MwEChT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "\n",
        "# Load the image\n",
        "image = load_img(image_path, target_size=(224, 224))\n",
        "image = img_to_array(image)\n",
        "\n",
        "# Preprocess the image\n",
        "image /= 255.0\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Make a prediction using the model\n",
        "prediction = model2.predict(image)\n",
        "\n",
        "# Assuming the prediction output is stored in the variable 'prediction'\n",
        "predicted_class = np.argmax(prediction)\n",
        "\n",
        "# To get the name of the predicted class, you can use the 'class_indices' attribute of the generator used for inference\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "predicted_class_name = class_names[predicted_class]\n",
        "\n",
        "print(predicted_class_name)"
      ],
      "metadata": {
        "id": "DKQsmZ15EFRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}